{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YibCLoSLRHp"
   },
   "source": [
    "Copyright 2018 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMykUGMauh9b"
   },
   "source": [
    "# Evaluation code\n",
    "\n",
    "\n",
    "__Disclaimer__\n",
    "*   This notebook contains experimental code, which may be changed without notice.\n",
    "*   The ideas here are some ideas relevant to fairness - they are not the whole story!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import getpass\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pkg_resources\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils_export.dataset import Dataset, Model\n",
    "from utils_export import utils_cloudml\n",
    "from utils_export import utils_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ['GCS_READ_CACHE_MAX_SIZE_MB'] = '0' #Faster to access GCS file + https://github.com/tensorflow/tensorflow/issues/15530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User inputs\n",
    "PROJECT_NAME = 'wikidetox'\n",
    "\n",
    "TEXT_FEATURE_NAME = 'comment_text' #Input text\n",
    "SENTENCE_KEY = 'comment_key' #Input key\n",
    "LABEL_NAME_PREDICTION_MODEL = 'frac_neg/logistic' # Output prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Creating input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text, lowercase=True):\n",
    "  \"\"\"Converts text to a list of words.\n",
    "\n",
    "  Args:\n",
    "    text: piece of text to tokenize (string).\n",
    "    lowercase: whether to include lowercasing in preprocessing (boolean).\n",
    "    tokenizer: Python function to tokenize the text on.\n",
    "\n",
    "  Returns:\n",
    "    A list of strings (words).\n",
    "  \"\"\"\n",
    "  words = nltk.word_tokenize(text.decode('utf-8'))\n",
    "  if lowercase:\n",
    "    words = [w.lower() for w in words]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User inputs\n",
    "PERFORMANCE_DATASET = 'gs://kaggle-model-experiments/resources/toxicity_q42017_test.tfrecord'\n",
    "LABEL_NAME_TEST_FILE = 'frac_neg' #Name of the label in the performance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define features\n",
    "\n",
    "# DECODING\n",
    "decoding_input_features = {\n",
    "  TEXT_FEATURE_NAME: tf.FixedLenFeature([], dtype=tf.string),\n",
    "  LABEL_NAME_TEST_FILE: tf.FixedLenFeature([], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "def input_fn_performance(max_n_examples=None, random_filter_keep_rate=1.0):\n",
    "    res = utils_tfrecords.decode_tf_records_to_pandas(\n",
    "        decoding_input_features,\n",
    "        PERFORMANCE_DATASET,\n",
    "        max_n_examples,\n",
    "        random_filter_keep_rate)\n",
    "    res[TEXT_FEATURE_NAME] = list(map(tokenizer, res[TEXT_FEATURE_NAME]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U -q git+https://github.com/conversationai/unintended-ml-bias-analysis@1de676a31de9e43892964f71d1e38e90fc8b331e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unintended_ml_bias import model_bias_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading it from it the unintended_ml_bias github.\n",
    "entire_test_bias_df = pd.read_csv(\n",
    "    pkg_resources.resource_stream(\"unintended_ml_bias\", \"eval_datasets/bias_madlibs_77k.csv\"))\n",
    "entire_test_bias_df['raw_text'] = entire_test_bias_df['Text']\n",
    "entire_test_bias_df['label'] = entire_test_bias_df['Label']\n",
    "entire_test_bias_df['label'] = list(map(lambda x: x=='BAD', entire_test_bias_df['label']))\n",
    "entire_test_bias_df = entire_test_bias_df[['raw_text', 'label']].copy()\n",
    "terms = [line.strip()\n",
    "         for line in pkg_resources.resource_stream(\"unintended_ml_bias\", \"bias_madlibs_data/adjectives_people.txt\")]\n",
    "model_bias_analysis.add_subgroup_columns_from_text(entire_test_bias_df, 'raw_text', terms)\n",
    "# Add preprocessing\n",
    "entire_test_bias_df['text'] = list(map(tokenizer, entire_test_bias_df['raw_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_fn_bias(max_n_examples):\n",
    "    if max_n_examples:\n",
    "        res = entire_test_bias_df.sample(n=max_n_examples, random_state=2018)\n",
    "    else:\n",
    "        res = entire_test_bias_df\n",
    "    res = res.copy(deep=True)\n",
    "    res = res.rename(\n",
    "        columns={\n",
    "            'text': TEXT_FEATURE_NAME\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Running prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAMES = [\n",
    "    'tf_gru_attention_continuous:v_1537828514',\n",
    "    'tf_gru_attention_continuous:v_1537828537',\n",
    "    'tf_gru_attention_continuous:v_1537828585',\n",
    "    'tf_gru_attention_continuous:v_1537828630',\n",
    "    'tf_gru_attention_continuous:v_1537828675',\n",
    "    'tf_gru_attention_continuous:v_1537828722',\n",
    "    'tf_gru_attention_continuous:v_1537828745',\n",
    "]\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User inputs.\n",
    "model_input_spec = {\n",
    "    TEXT_FEATURE_NAME: utils_tfrecords.EncodingFeatureSpec.LIST_STRING} #library will use this automatically\n",
    "\n",
    "model = Model(\n",
    "    feature_keys_spec=model_input_spec,\n",
    "    prediction_keys=LABEL_NAME_PREDICTION_MODEL,\n",
    "    example_key=SENTENCE_KEY,\n",
    "    model_names=MODEL_NAMES,\n",
    "    project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User inputs\n",
    "SIZE_PERFORMANCE_DATA_SET = 10000\n",
    "\n",
    "# Pattern for path of tf_records\n",
    "TF_RECORD_PERFORMANCE_PATTERN = os.path.join(\n",
    "    'gs://kaggle-model-experiments/',\n",
    "    getpass.getuser(),\n",
    "    'tfrecords/test_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_performance = Dataset(input_fn_performance)\n",
    "dataset_performance.load_data(SIZE_PERFORMANCE_DATA_SET, random_filter_keep_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_performance.add_model_prediction_to_data(model, tf_record_path_pattern=TF_RECORD_PERFORMANCE_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User inputs\n",
    "SIZE_BIAS_DATA_SET = None\n",
    "\n",
    "# Pattern for path of tf_records\n",
    "TF_RECORD_BIAS_PATTERN = os.path.join(\n",
    "    'gs://kaggle-model-experiments/',\n",
    "    getpass.getuser(),\n",
    "    'tfrecords/bias_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_bias = Dataset(input_fn_bias)\n",
    "dataset_bias.load_data(SIZE_BIAS_DATA_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_bias.add_model_prediction_to_data(model, tf_record_path_pattern=TF_RECORD_BIAS_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_bias.show_data().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_performance.show_data().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting the table to match the required format.\n",
    "test_performance_df = dataset_performance.show_data()\n",
    "test_performance_df = test_performance_df.rename(\n",
    "    columns={\n",
    "        #TEXT_FEATURE_NAME: 'raw_text',\n",
    "        LABEL_NAME_TEST_FILE: 'label'\n",
    "    })\n",
    "test_performance_df['label'] = list(map(lambda x :bool(round(x)), list(test_performance_df['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_bias_df = dataset_bias.show_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8m8QI4qEjtcY"
   },
   "source": [
    "# Part 3: Run evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhwSHsMtO9fF"
   },
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our performance data is in DataFrame df, with columns:\n",
    "\n",
    "text: Full text of the comment.\n",
    "label: True if the comment is Toxic, False otherwise.\n",
    "< model name >: One column per model, cells contain the score from that model.\n",
    "You can run the analysis below on any data in this format. Subgroup labels can be generated via words in the text as done above, or come from human labels if you have them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XUZYCq-6N8MK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1530641399913,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "yc8SWZbqMwA4",
    "outputId": "6e9399b8-ce22-42bb-c318-959bae73f6c0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_family in [MODEL_NAMES]:\n",
    "  auc_list = []\n",
    "  for _model in model_family:\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        test_performance_df['label'],\n",
    "        test_performance_df[_model]\n",
    "    )\n",
    "    auc_model = metrics.auc(fpr, tpr)\n",
    "    auc_list.append(auc_model)\n",
    "    print ('Auc for model {}: {}'.format(_model, auc_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_family in [MODEL_NAMES]:\n",
    "  auc_list = []\n",
    "  for _model in model_family:\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        test_bias_df['label'],\n",
    "        test_bias_df[_model]\n",
    "    )\n",
    "    auc_model = metrics.auc(fpr, tpr)\n",
    "    auc_list.append(auc_model)\n",
    "    print ('Auc for model {}: {}'.format(_model, auc_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTrKsfIcxoBh"
   },
   "source": [
    "## Unintended Bias Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "plt.style.use(u'ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "performance_aucs = []\n",
    "pinned_auc_equality_differences = []\n",
    "pinned_auc_subgroup_males = []\n",
    "pinned_auc_subgroup_females = []\n",
    "pinned_auc_subgroup_males_vs_females = []\n",
    "for _model in MODEL_NAMES:\n",
    "    \n",
    "    # Get timestamp\n",
    "    timestamp = _model.split(':')[1].replace('v_', '')\n",
    "    timestamps.append(timestamp)\n",
    "    \n",
    "    # Auc\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        test_performance_df['label'],\n",
    "        test_performance_df[_model]\n",
    "    )\n",
    "    auc_model = metrics.auc(fpr, tpr)\n",
    "    performance_aucs.append(auc_model)\n",
    "    \n",
    "    # Pinned  AUC equality difference\n",
    "    pinned_auc_equality_difference = model_bias_analysis.per_subgroup_auc_diff_from_overall(\n",
    "        test_bias_df, terms, [[_model]], squared_error=False)\n",
    "    pinned_auc_equality_differences.append(pinned_auc_equality_difference['pinned_auc_equality_difference'].values[0])\n",
    "    \n",
    "    # Pinned Auc for subgroups\n",
    "    pinned_auc_subgroup_male = model_bias_analysis.per_subgroup_aucs(\n",
    "        test_bias_df, ['male'], [[_model]], 'label')\n",
    "    pinned_auc_subgroup_males.append(pinned_auc_subgroup_male[_model + '_aucs'].values[0][0])\n",
    "    pinned_auc_subgroup_female = model_bias_analysis.per_subgroup_aucs(\n",
    "        test_bias_df, ['female'], [[_model]], 'label')\n",
    "    pinned_auc_subgroup_females.append(pinned_auc_subgroup_female[_model + '_aucs'].values[0][0])\n",
    "    \n",
    "    pinned_auc_subgroup_males_vs_female = pinned_auc_equality_difference['pinned_auc_equality_difference'].values[0] - pinned_auc_subgroup_female[_model + '_aucs'].values[0][0]\n",
    "    pinned_auc_subgroup_males_vs_females.append(pinned_auc_subgroup_males_vs_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualization_dataframe = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'auc': performance_aucs,\n",
    "    'pinned_auc_eq_diff': pinned_auc_equality_differences,\n",
    "    'pinned_auc_sub_males': pinned_auc_subgroup_males,\n",
    "    'pinned_auc_sub_females': pinned_auc_subgroup_females,\n",
    "    'pinned_auc_subg_males_vs_female': pinned_auc_subgroup_males_vs_females,\n",
    "})\n",
    "visualization_dataframe = visualization_dataframe.sort_values(['timestamp'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 2))\n",
    "plt.plot(visualization_dataframe['timestamp'], visualization_dataframe['auc'], label='auc', color='k')\n",
    "plt.legend(prop={'size': 10}, loc=4)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13, 2))\n",
    "plt.plot(visualization_dataframe['timestamp'], visualization_dataframe['pinned_auc_eq_diff'], label='pinned_auc_eq_diff', color='r')\n",
    "plt.legend(prop={'size': 10}, loc=4)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13, 2))\n",
    "plt.plot(visualization_dataframe['timestamp'], visualization_dataframe['pinned_auc_sub_males'], label='pinned_auc_sub_males', color='g')\n",
    "plt.legend(prop={'size': 10}, loc=4)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13, 2))\n",
    "plt.plot(visualization_dataframe['timestamp'], visualization_dataframe['pinned_auc_sub_females'], label='pinned_auc_sub_females', color='b')\n",
    "plt.legend(prop={'size': 10}, loc=4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 2))\n",
    "plt.plot(visualization_dataframe['timestamp'], visualization_dataframe['pinned_auc_subg_males_vs_female'], label='pinned_auc_subg_males_vs_female', color='c')\n",
    "plt.legend(prop={'size': 10}, loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "jigsaw-evaluation-pipeline.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
